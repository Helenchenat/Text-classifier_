{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "749bc3ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import random\n",
    "import string\n",
    "from nltk import word_tokenize\n",
    "from nltk import FreqDist\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0043f18b",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_DIR=\"C:/Users/97455/Downloads/exercise_task/exercise_task/data/train\"\n",
    "LABELS=['business','entertainment','politics','sport','tech']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b98b001d",
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words=set(stopwords.words('english'))\n",
    "stop_words.add('said')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72b61789",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the spreadsheet with category and text columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "814d031b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_set():\n",
    "    with open('data.txt','w',encoding='utf8') as outfile:\n",
    "        for label in LABELS:\n",
    "            dir='%s%s' % (BASE_DIR,label)\n",
    "            for filename in os.listdir(dir):\n",
    "                fullfilename='%s%s' % (dir,filename)\n",
    "                print(fullfilename)\n",
    "                with open(fullfilename,'rb') as file:\n",
    "                    text=file.read().decode(errors='replace').replace('\\n','')\n",
    "                    outfile.write('%s\\t%s\\t%s\\n' % (label,filename,text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7351d7b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitting each words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "434f565c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_docs():\n",
    "    docs=[]\n",
    "    with open('data.txt','r', encoding='utf8') as datafile:\n",
    "        for row in datafile:\n",
    "            parts= rows.split['\\t']\n",
    "            doc=(parts[0],parts[2].strip())\n",
    "            docs.append(doc)\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "29227177",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making the list of all word for each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "d919a5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    #remove punctuation\n",
    "    text=text.translate(str.maketrans('','',string.punctuation))\n",
    "    #convert to lowecase\n",
    "    text=text.lowe()\n",
    "    return(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "3c62b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_frequency_dist(docs):\n",
    "    tokens=defaultdict(list)\n",
    "    \n",
    "    for doc in docs:\n",
    "        doc_label=doc[0]\n",
    "        doc_text=clean_text[1]\n",
    "        \n",
    "        doc_tokens=word_tokenize(doc_text)\n",
    "        tokens[doc_label].extend(doc_tokens)\n",
    "        \n",
    "    for category_label,category_tokens in tokens.items():\n",
    "        print(category_label)\n",
    "        fd=FreqDist(category_tokens)\n",
    "        print(fd.most_common(20))\n",
    "        \n",
    "        \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "faee17a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_splits(docs):\n",
    "    #scramble docs\n",
    "    random.shuffle(docs)\n",
    "    \n",
    "    x_train=[]\n",
    "    y_train=[]\n",
    "    x_test=[]\n",
    "    y_test=[]\n",
    "    pivot=int(.80*len(docs))\n",
    "    \n",
    "    for i in range(0,pivot):\n",
    "        x_train.append(docs[i][1])\n",
    "        y_train.append(docs[i][0])\n",
    "    for i in range(0,len(docs)):\n",
    "        x_test.append(docs[i][1])\n",
    "        y_test.append(docs[i][0])\n",
    "    return x_train,x_test,y_train,y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0fc4473c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_classifier(title,classifier,vectorizer,x_test,y_test):\n",
    "    x_test_tfidf=vectorizer.transform(x_test)\n",
    "    y_pred=classifier.predict(x_test_tfidf)\n",
    "\n",
    "    \n",
    "    precision=metrics.precision_score(y_test,y_pred)\n",
    "    recall=metrics.recall_score(y_test,y_pred)\n",
    "    f1=metrics.f1_score(y_test,y_pred)\n",
    "    \n",
    "    print(\"%s\\t%f\\t%f\\t%f\\n\") % (title,precision,recall,f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "6844e680",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_classifier(docs):\n",
    "    x_train,x_test,y_train,y_test=get_splits(docs)\n",
    "    vectorizer=CountVectorizer(stop_words='english',ngram_range=(1,3),min_df=3,analyzer='word')\n",
    "    #create doc-term matrix\n",
    "    dtm= vectorizer.fit_transform(x_train)\n",
    "    # train naive bayes classifier\n",
    "    naive_bayes_classifier=MultinomialNB().fit(dtm,y_train)\n",
    "    evaluate_classifier(\"Naive Bayes\\tTRAIN\\t\",naive_bayes_classifier,vectorizer,x_train)\n",
    "    evaluate_classifier(\"Naive Bayes\\tTRAIN\\t\",naive_bayes_classifier,vectorizer,x_test)\n",
    "    # store the classifier\n",
    "    clf_file_name='naive_bayes_classifier.pk1'\n",
    "    pickle.dump(naive_bayes_classifier,open(clf_filename,'wb'))\n",
    "    # storing vectorizer inorder to transform new data\n",
    "    vec_filename='count_vectorizer.pk1'\n",
    "    pickle.dump(vectorizer,open(vec_filename,'wb'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6febbcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify(text):\n",
    "    clf_filename='naive_bayes_classifier.pk1'\n",
    "    nb_clf=pickle.load(open(clf_filename,'rb'))\n",
    "    \n",
    "    #vectorize the new text\n",
    "    vec_filename='count_vectorizer.pk1'\n",
    "    vectorizer=pickle.load(open(vec_filename,'rb'))\n",
    "    \n",
    "    pred=nb_clf.predict(vectorizer.transform(text))\n",
    "    \n",
    "    print(pred[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f26555cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
